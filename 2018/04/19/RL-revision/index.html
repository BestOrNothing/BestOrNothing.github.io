<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>RL revision | Zhensong Yan&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="My RL revision notes.">
<meta property="og:type" content="article">
<meta property="og:title" content="RL revision">
<meta property="og:url" content="http://yoursite.com/2018/04/19/RL-revision/index.html">
<meta property="og:site_name" content="Zhensong Yan&#39;s Blog">
<meta property="og:description" content="My RL revision notes.">
<meta property="og:image" content="http://yoursite.com/2018/04/19/image/RL_revision/matrix_func.png">
<meta property="og:image" content="http://yoursite.com/2018/04/19/image/RL_revision/shapley.png">
<meta property="og:updated_time" content="2018-04-30T10:44:40.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL revision">
<meta name="twitter:description" content="My RL revision notes.">
<meta name="twitter:image" content="http://yoursite.com/2018/04/19/image/RL_revision/matrix_func.png">
  
    <link rel="alternate" href="/atom.xml" title="Zhensong Yan&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zhensong Yan&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-RL-revision" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/19/RL-revision/" class="article-date">
  <time datetime="2018-04-19T09:37:01.000Z" itemprop="datePublished">2018-04-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      RL revision
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>My RL revision notes.</p>
<a id="more"></a>
<h1 id="Generally"><a href="#Generally" class="headerlink" title="Generally"></a>Generally</h1><ul>
<li>What are the following?<ul>
<li>action space: the space of possible actions.</li>
<li>state space: all possible states.</li>
<li>transition function: Given a state $s$ and an action $a$, the probability of each possible next state $s’$.</li>
<li>reward function: Given a state $s$ and an action $a$, the expected reward of the next reward.</li>
<li>discount factor: $\gamma$.</li>
<li>policy: a distribution over actions.</li>
<li>return: the sum of all the rewards from the current time step.</li>
<li>value function: state-value, expected return; action-value, the expected return after taking action $a$.</li>
</ul>
</li>
<li>Explain the Markov Property (with equations and conceptually).<ul>
<li>The next state and action only depends on the current state and action.</li>
<li>$Pr[s_{t+1}=s’,r_{t+1}=r\mid s_t,a_t,r_t,…,r_1,s_0,a_0] = Pr[s_{t+1}=s’,r_{t+1}=r\mid s_t,a_t]$</li>
</ul>
</li>
<li>What does it mean to evaluate a policy? Equivalently, what is the goal of Policy Evaluation? Or what is the RL prediction task?<ul>
<li>Evaluate state-value and action-value. </li>
</ul>
</li>
<li>What is the Reinforcement Learning (RL) control problem?<ul>
<li>Find the optimal policy.</li>
</ul>
</li>
<li>Write a reward or transition function in matrix format.<ul>
<li><img src="../image/RL_revision/matrix_func.png" alt=""></li>
</ul>
</li>
<li>Draw a transition graph.</li>
<li>Write a policy in matrix format.</li>
<li>Execute a given policy on paper.</li>
<li>What are the differences between episodic and continuing tasks, and what role does the discount factor play in either of them?<ul>
<li>Episodic tasks have absorbing state, which means it has end. In episodic tasks, there is no discount factor. In continuing tasks, the discount factor prevents it from infinite return.</li>
</ul>
</li>
<li>What is an absorbing/terminating state and what is the return from such a state?<ul>
<li>The return is 0.</li>
</ul>
</li>
<li>Explain the formulas defining the state (V ) and action (Q) value functions.</li>
<li>Explain the Bellman Equations, and the Bellman Optimality Equations, for V and Q.<ul>
<li>$V^\pi(s)=\sum_a \pi(s,a)\sum_{s’}P^a_{s,s’}[R^a_{s,s’}+\gamma V^\pi(s’)]$</li>
<li>$V^*(s) = \max_\pi V^\pi(s)$ for all $s\in S$</li>
<li>$V^<em>(s) = \max_a\sum_{s’}P^a_{ss’}[R^a_{ss’}+\gamma V^</em>(s’)]$</li>
<li>$Q^*(s,a)=\max_\pi Q^\pi(s,a)$, for all $s\in S$ and $a\in A$.</li>
<li>$Q^<em>(s,a)=\sum_{s’}P^a_{ss’}[R^a_{ss’}+\gamma\max_{a’}Q^</em>(s’,a’)]$</li>
</ul>
</li>
<li>Give the relation between V and Q under different conditions (stochastic versus deterministic policy, optimal versus non-optimal policy).<ul>
<li>stochastic vs. deterministic: for stochastic, $V^\pi(s)=\sum_a \pi(s,a)Q^\pi(s,a)$, otherwise, $V^\pi(s)=Q^\pi(s,a)$.</li>
<li>optimal vs. non-optimal: for optimal, $V^<em>(s)=\max_{a\in A(s)}Q^{\pi^</em>}(s,a)$</li>
</ul>
</li>
<li>Value functions are defined for a specific policy.</li>
<li>There exists a unique solution to the Bellman equations.</li>
<li>What do we mean by ’tabular’ RL?<ul>
<li>The value can be obtained by looking up table.</li>
</ul>
</li>
<li>What does it mean for an RL algorithm to be model-based or model-free?<ul>
<li>model-based: DP</li>
<li>model-free: MC, TD</li>
</ul>
</li>
<li>For the tabular case, and if we known the model, we can solve the Bellman Equations as a linear programming problem of |S| number of equations and unknowns (where |S| is the number of states).</li>
<li>What is a backup operation in the context of RL?<ul>
<li>The operation transfers value information back to a state from its successor states.</li>
</ul>
</li>
<li>What is a ”full backup”?<ul>
<li>All the next states are backed up.</li>
</ul>
</li>
<li>Explain the impact of the learning rate, generally and specifically for the tabular case.<ul>
<li>If learning rate is too large, the algorithm won’t converge. If it is too small, the algorithm will learn very slowly and possibly get stuck in the local minimum.</li>
</ul>
</li>
<li>What is ”sampling”?<ul>
<li>Generate an episode using the current policy.</li>
</ul>
</li>
<li>What does it mean to ”bootstrap”?<ul>
<li>Generating estimates from other estimates.</li>
</ul>
</li>
<li>A Markov Decision Process (MDP) with a defined policy defines a Markov Chain (what is a Markov Chain?).<ul>
<li>Markov Chain does not have action.</li>
</ul>
</li>
<li>Explain the concept of Generalised Policy Iteration (GPI).<ul>
<li>Policy evaluation</li>
<li>Policy improvement</li>
</ul>
</li>
<li>Are any GPI algorithms guarantee to converge to an optimal policy? No.</li>
<li>Model a described problem as an MDP/SMDP/POMDP, identify what if any information is missing or ill-specified, and assume values for any missing info such that they are consistent with the problem. Explain those assumptions in terms of how a learned policy would behave online.</li>
<li>Explain the Difference between Temporal Difference (TD), TD($\lambda$), Monte Carlo, and Dynamic Programming, with the use of backup diagrams.</li>
<li>Why would you use?<ul>
<li>Dynamic Programming</li>
<li>Monte Carlo</li>
<li>Temporal Difference</li>
</ul>
</li>
<li>What algorithms are guaranteed to converge and under what conditions?</li>
<li>Given the description of a problem, pick an algorithm to solve it, and argue for your choice, while still pointing out any negatives.</li>
<li>Can we generally define an optimal policy if we are given the respective state-value function?</li>
</ul>
<h1 id="Dynamic-programming"><a href="#Dynamic-programming" class="headerlink" title="Dynamic programming"></a>Dynamic programming</h1><ul>
<li>Dynamic Programming algorithms are model-based.</li>
<li>Understand Iterative Policy Evaluation in detail and be able to explain it and run a few steps.</li>
<li><p>Explain the Policy Improvement Theorem.</p>
<ul>
<li><p>Let $\pi$ and $\pi’$ be any pair of deterministic policies such that </p>
<p>$$Q^\pi(s,\pi’(s))\ge V^\pi(s), \forall s\in S$$</p>
</li>
<li>Then the policy $\pi’$ must be as good as, or better than, $\pi$.</li>
</ul>
</li>
<li>Do a policy improvement step (greedy or $\epsilon$-greedy).</li>
<li>Understand Policy Iteration Iteration in detail and be able to explain it and run a few steps.<ul>
<li>Policy evaluation</li>
<li>Policy improvement procedure by looking one step ahead (find $\arg \max_a$). </li>
</ul>
</li>
<li><p>Understand Value Iteration in detail and be able to explain it and run a few steps.</p>
<p>  $$V_{k+1}(s)=\max_a E[r_{r+1}+\gamma V_k(s_{t+1})\mid s_t=s,a_t=a]=\max_a\sum_{s’}P^a_{ss’}[R^a_{ss’}+\gamma V_k(s’)]$$</p>
</li>
<li>Explain how Value Iteration relates to Policy Iteration.<ul>
<li>Value iteration effectively combines one sweep of policy evaluation and one sweep of policy improvement in each of its sweeps.</li>
</ul>
</li>
<li>Explain Asynchronous Dynamic Programming. Recognise an example algorithm as being an instance of Asynchronous Dynamic Programming.<ul>
<li>Update values in any order.<h1 id="Monte-Carlo"><a href="#Monte-Carlo" class="headerlink" title="Monte Carlo"></a>Monte Carlo</h1></li>
</ul>
</li>
<li>Monte Carlo (MC) RL is model-free.</li>
<li>MC RL does not bootstrap.</li>
<li>MC uses samples from real-world experience or simulation.</li>
<li>Though a ”simulator” is a model, it does not necessarily explicitly define a transition and reward function (which is what we are concerned with when calling a procedure ”model-free” or ”model-based”).</li>
<li>Explain the difference between first and every visit MC. Given a set of trajectories, produce the samples under either method.<ul>
<li>first visit method average the returns at the first time encountering a state in the episode.</li>
</ul>
</li>
<li>Given one or more sampled trajectories, execute a requested MC algorithm for prediction or control.</li>
<li>Explain the idea behind exploring starts.<ul>
<li>When evaluating action-value, many state action pairs are not visited. So we explore starts to make sure we can visit those states.</li>
</ul>
</li>
<li>Explain MC control as an instantiation of Generalised Policy Iteration.</li>
<li>Infinite Sampling and Exploring All States are not realistic assumptions in most real applications.</li>
<li>Explain the concepts of on-policy and off-policy.<ul>
<li>On-policy methods estimate the value of a policy while using it for control</li>
<li>In off-policy methods, these two functions are separated. The <em>behavior policy</em> is unrelated to the policy (<em>estimation policy</em>) that is evaluated and improved. An advantage of this separation is that the estimation policy may be deterministic, while the behavior policy can continue to sample all possible actions.</li>
</ul>
</li>
<li>Explain $\epsilon$-greedy action selection/policies.<ul>
<li>With probability $\epsilon$, we choose the next action greedily. With probability $1-\epsilon$, we choose the next action at random.</li>
</ul>
</li>
<li>Do a step of policy improvement for an $\epsilon$-greedy policy.</li>
<li>Understand On-Policy MC Control in detail and be able to explain it and run a few steps (given example trajectories).</li>
<li>Understand On-Policy MC Control in detail and be able to explain it and run a few steps (given example trajectories).</li>
<li>What are the Estimation and Behaviour policies, in the context of an off-policy RL algorithm?<ul>
<li>see above</li>
</ul>
</li>
<li>Understand the incremental implementation of Off-Policy MC control.</li>
<li>Different ways to achieve exploration in MC (and RL in general):<ul>
<li>on-policy that still explores</li>
<li>off-policy (decoupling exploration from evaluation)</li>
<li>exploring starts</li>
</ul>
</li>
<li>Understand MC sample-averaging as learning rate.</li>
<li>MC is more robust towards violations of the Markov assumption, since it doesn’t bootstrap.<h1 id="Temporal-difference"><a href="#Temporal-difference" class="headerlink" title="Temporal difference"></a>Temporal difference</h1></li>
<li>TD learning does not require a model.</li>
<li>TD learning bootstraps.</li>
<li>Understand TD(0) in detail and be able to explain it and run a few steps.</li>
<li>What is the ”target” of an update procedure/backup?</li>
<li>Why might TD converge faster than MC? (It moves towards a better estimate that takes transitions into account, or it bootstraps. MC better matches the training data).</li>
<li>Explain on-policy and off-policy TD control differences.<ul>
<li>on-policy: backup the value of the next action</li>
<li>off-policy: backup the maximal value of the next action but do not necessarily take it (with probability $1-\epsilon$ take the next action at random, which causes danger (eg. cliff walking)).</li>
</ul>
</li>
<li>Understand SARSA in detail and be able to explain it and run a few steps (given example trajectories).</li>
<li>Understand Q-Learning in detail and be able to explain it and run a few steps (given example trajectories).</li>
<li>Explain the difference between the on-line performance of SARSA and Q-Learning (with or without an example application).<ul>
<li>Q-learning directly learns the optimal policy.</li>
<li>Q-learning (and off-policy learning in general) has higher per-sample variance than SARSA.</li>
<li>SARSA is more conservative.</li>
<li>In practice the last point can make a big difference if mistakes are costly - e.g. you are training a robot not in simulation, but in the real world. You may prefer a more conservative learning algorithm that avoids high risk, if there was real time and money at stake if the robot was damaged.<br>*If your goal is to train an optimal agent in simulation, or in a low-cost and fast-iterating environment, then Q-learning is a good choice, due to the first point (learning optimal policy directly). If your agent learns online, and you care about rewards gained whilst learning, then SARSA may be a better choice.<h1 id="Eligibility-traces"><a href="#Eligibility-traces" class="headerlink" title="Eligibility traces"></a>Eligibility traces</h1></li>
</ul>
</li>
<li>Compute an n-step return (also show in backup diagram).</li>
<li><em>n</em>-step return when episode ends in less than <em>n + 1</em> steps is full return.</li>
<li>Understand n-step TD evaluation (this is not TD($\lambda$)!!) in detail and be able to explain it and run a few steps (given example trajectories).</li>
<li>Explain complex backups, and give an example.</li>
<li>Explain the formula for $\lambda$-return.<ul>
<li>$R_t^\lambda = (1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}R_t^{(n)}$</li>
</ul>
</li>
<li>Understand the $\lambda$-return algorithm in detail and be able to explain it and run a few steps (given example trajectories).</li>
<li>$\lambda$-return is not typically called TD($\lambda$).</li>
<li>What is the main problem with the $\lambda$-return algorithm (and n-step TD)?<ul>
<li>it is off-line. The disadvantage of off-line algorithm. During the episode, if something bad happened, the agent cannot learn it.</li>
</ul>
</li>
<li>Explain the relation between the forward and backward view of TD($\lambda$).<ul>
<li>They are actually doing the same job, but forward view updates the current state-value in terms of the future state. Backward view updates all the previous state-values when at the current state.</li>
</ul>
</li>
<li>explain the concept of eligibility traces.<ul>
<li>it measures the recency of a state.</li>
</ul>
</li>
<li>Iteratively update the eligibility traces of states given a sampled trajectory.</li>
<li>Explain the $\lambda$ parameter, and compare it with the discount factor.<ul>
<li>P. 172. trace-decay</li>
</ul>
</li>
<li>Understand SARSA($\lambda$) in detail and be able to explain it and run a few steps (given example trajectories).</li>
<li>What is the difficulty with implementing Q($\lambda$)?<ul>
<li>not sure. P. 189?</li>
</ul>
</li>
</ul>
<h1 id="Function-approximation"><a href="#Function-approximation" class="headerlink" title="Function approximation"></a>Function approximation</h1><ul>
<li>Explain the concept of Function Approximation for value functions in RL.<ul>
<li>Function approximation is an instance of supervised learning.</li>
<li>state features and parameters together come up with a prediction</li>
</ul>
</li>
<li>What are some reasons why we might choose to use function approximation in RL?<ul>
<li>Large state space.</li>
</ul>
</li>
<li>Given the description of an RL problem, suggest a set of features to describe states with.</li>
<li>Given the representation of a linear value function as weights, and a state vector, compute the value function at that state.</li>
<li>Give and explain the general formula for doing backups with function approximation.</li>
<li>Do an update given a target, a proposed RL algorithm, and a linear representation of the value function.</li>
<li>TD and MC can be applied to RL problems with function approximation, as long as we can compute or estimate partial derivatives of the value function over the approximate value function parameters.</li>
<li>The MC solution is gradient descent.</li>
<li>The TD solution approximates/is like gradient descent.</li>
<li>Explain the concept of Coarse Coding.<ul>
<li>P. 202</li>
<li>like one-hot encoding, the state is in which circle, and then which circle is labelled as one.</li>
</ul>
</li>
</ul>
<h1 id="Semi-MDPs-and-Options"><a href="#Semi-MDPs-and-Options" class="headerlink" title="Semi-MDPs and Options"></a>Semi-MDPs and Options</h1><ul>
<li>Explain the concept of Reward Shaping.<ul>
<li>Add rewards or penalties for achieving sub-goals/errors. Add progress estimator. Add intermediate rewards will potentially allow RL to handle more complex problems.</li>
</ul>
</li>
<li>Main problem with reward shaping is that it changes the problem solved.</li>
<li>What is a Semi-MDP (SMDP)?<ul>
<li>A generalization of MDPs: The amount of time between one decision and the next is a random variable.</li>
<li>It is defined in terms of:<ul>
<li>$P(s’,\tau\mid s,a)$, transition probability ($\tau$ is the waiting time between one decision and the next one)</li>
<li>$R(s,a)$, reward, amount expected to accumulate during waiting time, $\tau$, in particular state and action.</li>
</ul>
</li>
</ul>
</li>
<li>For SMDPs, the Markov assumption holds at states -when- a decision is made. The Markov assumption does not necessarily hold in-between time-steps.</li>
<li>The time between decisions can be discrete or continuous.</li>
<li>Explain the Bellman Equations for SMDPs.<ul>
<li>Bellman equation can be then written as $V^<em>(s) = \max_{a\in A_s} [R(s,a) + \sum_{s’,\tau}\gamma^\tau P(s’,\tau\mid s,a)V^</em>(s’)]$. We need to sum over waiting time.</li>
<li>The equation for action value can be written down as: $Q^<em>(s,a) = R(s,a) + \sum_{s’,\tau}\gamma^\tau P(s’,\tau\mid s,a)\max_{a’\in A_s}Q^</em>(s’,a’)$</li>
<li>So dynamic programming can be naturally extended to SMDP as well.</li>
</ul>
</li>
<li>We can run DP, MC, and TD algorithms on SMDPs with few changes.</li>
<li><p>write the Q-Learning update for a discrete time SMDP</p>
<ul>
<li><p>Consider the standard Q-learning algorithm, rewritten slightly in the following form:</p>
<p>$$Q_{k+1}(s,a) = (1-\alpha_k)Q_k(s,a)+\alpha_k[r+\gamma\max_{a’\in A_s}Q_k(s’,a’)]$$</p>
</li>
<li><p>Modify the equation above:</p>
<p>$$Q_{k+1}(s,a) = (1-\alpha_k)Q_k(s,a)+\alpha_k[r_{t+1}+\gamma r_{t+2}+…+\gamma^{\tau-1}r_{t+\tau}+\gamma^\tau\max_{a’\in A_s}Q_k(s’,a’)]$$</p>
</li>
</ul>
</li>
<li>Explain the role of $\epsilon^{-l\tau}$ for continuous time SMDPs.<ul>
<li>discount factor</li>
</ul>
</li>
<li>Explain the concept of Options (motivation)<ul>
<li>Add temporally extended activities to choices available to RL agent, without precluding planning and learning at finer grained MDP level</li>
<li>Optimal policies over primitives are not compromised due to addition of options. However, if an option is useful, learning will quickly find this out – prevent prolonged and useless ‘flailing about’</li>
</ul>
</li>
<li>Define Options<ul>
<li>An option is a behavior defined in terms of: $o={I_o,\pi_o,\beta_o}$</li>
<li>$I_o$: Set of states in which $o$ can be initiated.</li>
<li>$\pi_o(s)$: Policy when $o$ is executing.</li>
<li>$\beta_o(s)$: Probability that $o$ terminates in $s$.</li>
<li>If the current state is $s$, the next action $a$ is selected with probability $\pi(s,a)$, the environment makes a transition to state $s’$, where the option either terminates with probability $\beta(s’)$ or else continues, determining the next action $a’$ with probability $\pi(s’,a’)$ and so on.</li>
</ul>
</li>
<li>Given a deterministic policy over options, and their definitions, write out the state-action trajectory as if the policy was being executed.</li>
<li>Give an example of a hierarchy of options.</li>
<li>Given a problem description, suggest a hierarchy of options.</li>
<li>We can learn optimal policies over options, but these are only optimal given the constraints the options impose on behavior. Procedures over SMDPs have the same guarantees as over MDPs, but with solutions con- strained by the options.</li>
<li>Given the definition of an option, and a set of states, answer questions such as:<ul>
<li>Can the option terminate in this state?</li>
<li>Can the option be initiated in this state?</li>
</ul>
</li>
<li>How do options define an SMDP?<ul>
<li>Option + MDP = SMDP</li>
</ul>
</li>
<li>Bellman Equations for Options<ul>
<li>$V_O^<em>(s)=\max_{o\in O_s}E[r+\gamma^k V_O^</em>(s’)\mid \epsilon(o,s)]$</li>
<li>(from the paper) $V^<em><em>O(s)=\max</em>{o\in O_s}[R(s,o)+\sum_{s’}P(s’\mid s,o)V^</em>_O(s’)]$</li>
<li>(from the paper) $Q^<em><em>O(s,o)=R(s,o)+\sum</em>{s’}P(s’\mid s,o)\max_{o’\in O_{s’}}Q^</em>_o(s’,o’)$</li>
</ul>
</li>
<li>Options are a good way of introducing hierarchies to RL problems.</li>
</ul>
<h1 id="Partially-Observable-MDPs"><a href="#Partially-Observable-MDPs" class="headerlink" title="Partially Observable MDPs"></a>Partially Observable MDPs</h1><ul>
<li>What dose partial observability mean in the context of RL?<ul>
<li>We don’t observe the whole state. The agent only knows a part of the current state. </li>
</ul>
</li>
<li>Explain what a belief state is.<ul>
<li>Since the state is not observable, the agent has to make its decisions based on the belief state which is a <strong>posterior distribution over states</strong>.</li>
</ul>
</li>
<li>Give and explain the Bellman Equations for POMDPs.<ul>
<li>Let $b$ be the belief of the agent about the state under consideration.</li>
<li>$V_T(b) = \gamma \max_u [r(b,u)+\int V_{T-1}(b’)p(b’\mid u,b)db’]$</li>
</ul>
</li>
<li>Compute the value of a belief state given the state value function.<ul>
<li>Same as above.</li>
</ul>
</li>
<li>The main problem with POMDPs is that the value function is defined over probability distributions.</li>
<li>For finite worlds (state, action, measurement spaces, observations, finite horizons), we can represent the value function as a piece-wise linear function.</li>
<li><p>Given a POMDP model, a current belief state, and an observation, compute the posterior belief state.</p>
<ul>
<li><p>Use Bayesian theorem. Assuming given a belief state $x_1$ and an observation $z_1$.</p>
<p>$$p(x_1\mid z_1) = \frac{p(z_1\mid x_1)p(x_1)}{p(z_1)}$$</p>
</li>
</ul>
</li>
<li>Read a piece-wise linear value function graph.</li>
<li>Explain the concept of ”pruning” and its necessity for POMDP Value Iteration.<ul>
<li>Some functions are not worthy considering because they are less than others at any point.</li>
<li>Because the number of linear components increase exponentially, pruning is very necessary.</li>
</ul>
</li>
<li>Given a small POMDP problem, and a belief state, give an optimal policy for a short horizon (possibly implicitly defined by terminating/absorbing states).</li>
</ul>
<h1 id="Inverse-Reinforcement-Learning"><a href="#Inverse-Reinforcement-Learning" class="headerlink" title="Inverse Reinforcement Learning"></a>Inverse Reinforcement Learning</h1><ul>
<li>We can use supervised learning to learn a policy directly, by using observations of an agent’s behaviour (i.e. trajectories). This is ”Behavioural Cloning”.</li>
<li>Explain the concept of Inverse RL (IRL).<ul>
<li>Given the observations, we need to determine the rewards.</li>
</ul>
</li>
<li>Why might we choose to do IRL instead of Behavioural Cloning?<ul>
<li>Problem with behavioral cloning: agent is inflexible, cannot change goals. Also, such a policy would perform poorly when more than present state is needed in order to perform (e.g., when environment is mildly non-Markovian due to context).</li>
</ul>
</li>
<li>We can solve the IRL problem, given an observed trajectory or a policy, by formulating a linear program:<ul>
<li>Explain the tabular-case formulation.</li>
<li>Explain the linear function approximation case formulation.</li>
</ul>
</li>
<li>There are multiple reward functions that can explain a given behaviour (trajectory or policy), so the linear programs include constraints to produce something more intuitive/useful. Explain how different requirements inform our linear program objective function (maximum difference on Q, and normalisation).<ul>
<li>maximum difference means we want the optimal policy as good as possible.</li>
<li>normalization means we do not want rewards to be too large.</li>
</ul>
</li>
<li>Why do we need a model for formulating the linear function approximation linear program?<ul>
<li>Because in real life state space sometimes is too large to be enumerated.</li>
</ul>
</li>
</ul>
<h1 id="Multi-agent-RL"><a href="#Multi-agent-RL" class="headerlink" title="Multi-agent RL"></a>Multi-agent RL</h1><ul>
<li>Read and write payoff matrices for 2-player games.</li>
<li>What are zero-sum and fully cooperative games?<ul>
<li>zero-sum: the sum of two agents’ payoffs is zero (constant payoff).</li>
<li>fully cooperative: payoffs are the same (common payoff).</li>
</ul>
</li>
<li>Understand and explain pure and mixed strategies.<ul>
<li>pure strategies: select an action</li>
<li>mixed strategies: select an action according to some probability distribution.</li>
</ul>
</li>
<li>Define a Stochastic Game.<ul>
<li>A stochastic game is a tuple $(n,S,A_{1…n},T,R_{1…n})$</li>
</ul>
</li>
<li>Explain Shapley’s [1953] Value Iteration algorithm.<ul>
<li><img src="../image/RL_revision/shapley.png" alt=""></li>
<li>Function value returns the expected value of playing the matrix game’s equilibrium.</li>
</ul>
</li>
<li>Given a problem description of an iterative zero-sum game, and a description of the current state, compute an optimal policy for your agent using minimax search.</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/19/RL-revision/" data-id="cjgm40ssi0007xoyrkrt8gfn0" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/04/19/Semantic-role-labelling/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          Semantic role labelling
        
      </div>
    </a>
  
  
    <a href="/2018/04/17/Dependency-parsing/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">Dependency parsing</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/machine-learning/" style="font-size: 10px;">machine learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/04/21/Inverse-Reinforcement-Learning/">Inverse Reinforcement Learning</a>
          </li>
        
          <li>
            <a href="/2018/04/19/Semantic-role-labelling/">Semantic role labelling</a>
          </li>
        
          <li>
            <a href="/2018/04/19/RL-revision/">RL revision</a>
          </li>
        
          <li>
            <a href="/2018/04/17/Dependency-parsing/">Dependency parsing</a>
          </li>
        
          <li>
            <a href="/2018/04/17/Eligibility-traces/">Eligibility traces</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Zhensong Yan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>