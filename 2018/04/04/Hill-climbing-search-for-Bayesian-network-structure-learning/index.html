<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>Hill-climbing search for Bayesian network structure learning | Zhensong Yan&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Recently I’m reading papers about incremental Bayesian network structure learning algorithm, so as to do my master dissertation. I therefore write my understanding of the algorithms in this blog.">
<meta name="keywords" content="machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Hill-climbing search for Bayesian network structure learning">
<meta property="og:url" content="http://yoursite.com/2018/04/04/Hill-climbing-search-for-Bayesian-network-structure-learning/index.html">
<meta property="og:site_name" content="Zhensong Yan&#39;s Blog">
<meta property="og:description" content="Recently I’m reading papers about incremental Bayesian network structure learning algorithm, so as to do my master dissertation. I therefore write my understanding of the algorithms in this blog.">
<meta property="og:updated_time" content="2018-04-03T10:46:56.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hill-climbing search for Bayesian network structure learning">
<meta name="twitter:description" content="Recently I’m reading papers about incremental Bayesian network structure learning algorithm, so as to do my master dissertation. I therefore write my understanding of the algorithms in this blog.">
  
    <link rel="alternate" href="/atom.xml" title="Zhensong Yan&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zhensong Yan&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Hill-climbing-search-for-Bayesian-network-structure-learning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/04/Hill-climbing-search-for-Bayesian-network-structure-learning/" class="article-date">
  <time datetime="2018-04-04T20:16:57.000Z" itemprop="datePublished">2018-04-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Hill-climbing search for Bayesian network structure learning
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Recently I’m reading papers about incremental Bayesian network structure learning algorithm, so as to do my master dissertation. I therefore write my understanding of the algorithms in this blog.<br><a id="more"></a></p>
<h1 id="Theory-Refinement-on-Bayesian-Networks"><a href="#Theory-Refinement-on-Bayesian-Networks" class="headerlink" title="Theory Refinement on Bayesian Networks"></a>Theory Refinement on Bayesian Networks</h1><p>This section is my notes of <a href="https://arxiv.org/pdf/1303.5709.pdf" target="_blank" rel="noopener"><em>Theory Refinement on Bayesian Networks</em></a>.</p>
<h2 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h2><p>$E$ means the information an expert gives about the structure of the network. $\prec$ means an ordering of variables. $\Pi_x$ means the set of parents of $x$. $Pr(\Pi\mid \prec,E)=\prod_{x\in X}Pr(\Pi_x\mid\prec,E)$</p>
<p>Combined Bayesian network: The set of reasonable parent structures and their conditional probability tables and associated statistics.<br>Let $P_x$ denote a set containing sets of reasonable parent variables for the variable $x$. Then the set of reasonable parent structures is given by cartesian product $\otimes_{x\in X}P_x$. </p>
<p>The posterior on each reasonable parent structure $\Pi$ is:</p>
<p>$$Pr(\Pi\mid Sample, \prec, E) = \prod_{x\in X} Pr(\Pi_x\mid Sample, \prec, E)$$</p>
<p>$$Pr(\Pi_x\mid Sample, \prec, E) \propto Pr(\Pi_x \mid \prec, E) \prod_{j\in v(\Pi_x)} \frac{Beta_{m_x}(n_{x=1|j}+\alpha_x,…,n_{x=m_x|j}+\alpha_x)}{Beta_{m_x}(\alpha_x,…,\alpha_x)}$$</p>
<p>To access all alternative parent set $\Pi_x\in P_x$ efficiently, they are stored in a lattice structure where subset and superset parent sets are linked together in a web, denoted the parent lattice for x. Only those parent sets with significant posterior probabilities are stored and linked. </p>
<p>The nodes in the lattice are labeled as alive, dead or asleep. <strong>Alive</strong> nodes represent the set of “reasonable” alternatives having high posteriors, and correspond to those parent sets in $P_x$. <strong>Dead</strong> nodes exist in the lattice as dead-end markers in the search space. They have been explored and forever determined as unreasonable and not to further expand. <strong>Asleep</strong> nodes are similar but are only considered unreasonable for now and may be made alive later on.</p>
<h2 id="Update-combined-network"><a href="#Update-combined-network" class="headerlink" title="Update combined network"></a>Update combined network</h2><p>When new data is available, we need to update the combined network, namely theory refinement.</p>
<p>In the case that we need to update the combined network within limited time, we can only update the parameters rather than the structure of the network. The parameters refer to the conditional probability tables and associated statistics explained in the last subsection. The structure of the network refers to $\Pi$. Time complexity is $O(P)$.</p>
<p>If we have additional time, we can update the structure. We firstly introduce a batch beam search algorithm. We have three parameters $C,D$ and $E$ such that $1&gt;C&gt;D&gt;E$. Let $BP$ denote the best posterior during the search process. In the lattice, the nodes with posterior less than $E\times BP$ are labeled as dead. The nodes with posterior greater than $D\times BP$ are open, which means they will be searched and expanded. The nodes with posterior greater than $C\times BP$ are labeled as alive. The remaining nodes are labeled as asleep.</p>
<h1 id="Hill-climbing-search-HCS"><a href="#Hill-climbing-search-HCS" class="headerlink" title="Hill-climbing search (HCS)"></a>Hill-climbing search (HCS)</h1><p>Hill-climbing search methods have three components:</p>
<ul>
<li>Objective function (aka. score function) denoted by $S(M,D)$, where $M$ is a model, and $D$ is a dataset. This function measures the quality of a given model.</li>
<li>Traverse operators denoted by $OP={op^1,…,op^k}$, every operator takes two arguments, and its return is a new model. $M’=op^i(M,A)$.</li>
<li>a domain of legal models denoted by $MD$.</li>
</ul>
<p>From its name we can see its idea. The search process is just like a hill-climbing process. In every step of search, we take the step that has maximal score (i.e. the step that improves the objective function most).</p>
<p>If explaining hill-climbing search in an more academic way, the idea is that at time step $t$, the model is $M_t$, and the next model is $M_{t+1} = argmax_M op^i(M,A)$, where $M\in MD$. The set of the models which can be reached in one step is call neighbor denoted by $N(M)$.</p>
<h1 id="Two-heuristics-that-transform-batch-HCS-to-incremental-HCS"><a href="#Two-heuristics-that-transform-batch-HCS-to-incremental-HCS" class="headerlink" title="Two heuristics that transform batch HCS to incremental HCS"></a>Two heuristics that transform batch HCS to incremental HCS</h1><h2 id="Why-incremental"><a href="#Why-incremental" class="headerlink" title="Why incremental?"></a>Why incremental?</h2><p>Why do we need incremental method rather than batch algorithm?</p>
<p>In real word, your data is updated every day, so you need to do incremental learning when new data is available. </p>
<h2 id="Assumption"><a href="#Assumption" class="headerlink" title="Assumption"></a>Assumption</h2><p>To apply these heuristics, we need to assume the objective function is continuous, so that if two datasets are similar, their score will not be very different.</p>
<h2 id="Traversal-Operators-in-Correct-Order-TOCO-heuristic"><a href="#Traversal-Operators-in-Correct-Order-TOCO-heuristic" class="headerlink" title="Traversal Operators in Correct Order (TOCO) heuristic"></a>Traversal Operators in Correct Order (TOCO) heuristic</h2><p>The idea of this heuristic is to only update the model when the order of the sequence of executed operators will be changed. If the order will be changed, it means the new data has significant influence on the objective function. In order to check if the order changes, we need to compute the values of the objective function, so we have to keep the sequence of operators and recompute them using new dataset $D_t={D_{t-1}\cup D’}$, where $D’$ is the new data. </p>
<h2 id="Reduced-Search-Space-RSS-heuristic"><a href="#Reduced-Search-Space-RSS-heuristic" class="headerlink" title="Reduced Search Space (RSS) heuristic"></a>Reduced Search Space (RSS) heuristic</h2><p>The idea of this heuristic is to reduce the search space by avoiding to explore those parts of the space where low quality models were found during former search steps. </p>
<p>RSS stores the nRSS operators and arguments pairs which are closest to the best one into a set $B$. In the next search steps, when new data are available, RSS restricts the search to the models of the neighborhood obtained with these pairs, namely $\mathbf{OpA}<em>{N(M)}\cap B$, where $\mathbf{OpA}</em>{N(M)}$ means the pairs of operators and arguments which can reach $N(M)$. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/04/Hill-climbing-search-for-Bayesian-network-structure-learning/" data-id="cjg9deiys00043ayrx4uq417v" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/04/05/Learning-the-structure-of-a-decision-network/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          Learning the structure of a decision network
        
      </div>
    </a>
  
  
    <a href="/2018/04/03/Factor-Analysis/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">Factor Analysis</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/machine-learning/" style="font-size: 10px;">machine learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/04/19/Semantic-role-labelling/">Semantic role labelling</a>
          </li>
        
          <li>
            <a href="/2018/04/19/RL-revision/">RL revision</a>
          </li>
        
          <li>
            <a href="/2018/04/17/Dependency-parsing/">Dependency parsing</a>
          </li>
        
          <li>
            <a href="/2018/04/17/Eligibility-traces/">Eligibility traces</a>
          </li>
        
          <li>
            <a href="/2018/04/05/Anime/">Anime</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Zhensong Yan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>